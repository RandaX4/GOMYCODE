{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2990b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.models import Sequential\n",
    "\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89366e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_to_train, path_to_test):\n",
    "    train_dataset = h5py.File(path_to_train)\n",
    "    train_x = np.array(train_dataset['train_set_x'][:])\n",
    "    train_y = np.array(train_dataset['train_set_y'][:])\n",
    "\n",
    "    test_dataset = h5py.File(path_to_test)\n",
    "    test_x = np.array(test_dataset['test_set_x'][:])\n",
    "    test_y = np.array(test_dataset['test_set_y'][:])\n",
    "\n",
    "    # y reshaped\n",
    "    train_y = train_y.reshape((1, train_x.shape[0]))\n",
    "    test_y = test_y.reshape((1, test_y.shape[0]))\n",
    "\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff8a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig = load_dataset(\"train_happy.h5\",\"test_happy.h5\")\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255\n",
    "\n",
    "# Reshape\n",
    "Y_train = Y_train_orig.T\n",
    "Y_test = Y_test_orig.T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbefe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig = load_dataset(\"train_happy.h5\",\"test_happy.h5\")\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255\n",
    "\n",
    "# Reshape\n",
    "Y_train = Y_train_orig.T\n",
    "Y_test = Y_test_orig.T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1944f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample image from dataset\n",
    "print(\"Image shape :\",X_train_orig[10].shape)\n",
    "imshow(X_train_orig[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d357dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,3,3,input_shape=(64,64,3)))\n",
    "model.add(Activation = 'relu')\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(32,3,3))\n",
    "model.add(Activation = 'relu')\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff18b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746bccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = history_model.history[accuracy]\n",
    "train_loss = history_model.history[loss]\n",
    "\n",
    "count = range(len(train_accuracy))\n",
    "plt.plot(count, train_accuracy, label='Training accuracy')\n",
    "plt.plot(count, train_loss, label='Training Loss')\n",
    "plt.title('epochs vs Training Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cedbfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "score=model.evaluate(X_test, Y_test)\n",
    "print('test accuracy:%.2f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
